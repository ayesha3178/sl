import org.apache.spark.{SparkConf, SparkContext}

object WordCount {
  def main(args: Array[String]): Unit = {
    // Create a Spark configuration and context
    val conf = new SparkConf().setAppName("Simple Word Count").setMaster("local[*]")
    val sc = new SparkContext(conf)

    // Load a text file (can replace with any path or HDFS path)
    val textFile = sc.textFile("sample.txt") // Make sure sample.txt exists

    // Split lines into words, map each word to (word, 1), then reduce by key
    val wordCounts = textFile
      .flatMap(line => line.split(" "))
      .map(word => (word, 1))
      .reduceByKey(_ + _)

    // Print the result
    wordCounts.collect().foreach {
      case (word, count) => println(s"$word: $count")
    }

    // Stop SparkContext
    sc.stop()
  }
}

/*
Theory Explanation:

What the program does:
This is a simple word count program written in Scala using Apache Spark. It processes a text file by loading it, splitting the lines into words, and counting how often each word appears. The program uses the MapReduce paradigm to process the data and provides an efficient way to count word occurrences, which is useful in various text processing and analytics tasks.

Key operations of the program:
1. **Spark Configuration and Context**:
   - The `SparkConf` object is created to set up the applicationâ€™s name and master (which in this case is `local[*]` meaning it runs on the local machine using all available cores).
   - A `SparkContext` is created using the configuration, which is the entry point for Spark functionality.
   
2. **Text File Loading**:
   - The program loads the text file (`sample.txt`) into an RDD (Resilient Distributed Dataset), which is Spark's primary data structure for parallel processing.
   
3. **Transformation**:
   - `flatMap`: It splits each line into words and flattens the result into a single list of words.
   - `map`: Each word is mapped to a tuple (word, 1) to represent a key-value pair.
   - `reduceByKey`: This is a transformation that groups the values by key (word) and reduces them by summing the occurrences of each word.

4. **Action**:
   - `collect`: It retrieves the results from the RDD and brings them to the driver (the local machine in this case).
   - The results are printed out, showing each word followed by its count.

5. **Stopping the Spark Context**:
   - The `sc.stop()` command stops the SparkContext to release resources after the computation is done.

Observations/Conclusions:
- **Input**: The input is a text file (in this case, `sample.txt`), where each line can contain multiple words.
- **Output**: The output is a list of word counts, with each word followed by the number of times it appears in the file.
  - Example output could look like:
    - `word1: 5`
    - `word2: 3`
    - `word3: 10`
- **Efficiency**: Spark's parallel processing makes the word count computation efficient even for large datasets. The `reduceByKey` operation ensures that the counting happens in a distributed and parallel manner, leading to better performance on large-scale data.
- **Scalability**: Although the program runs locally (using `local[*]`), it can easily be adapted to run on a distributed Spark cluster by changing the `setMaster` parameter to a cluster URL, which makes it scalable to large datasets stored in distributed file systems like HDFS.

*/
