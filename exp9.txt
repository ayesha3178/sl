import numpy as np

# Sigmoid and its derivative
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
def sigmoid_derivative(x):
    return x * (1 - x)

# Training data for XOR
X = np.array([[0,0],
              [0,1],
              [1,0],
              [1,1]])

y = np.array([[0],
              [1],
              [1],
              [0]])

# Seed for reproducibility
np.random.seed(42)

# Initialize weights and biases
input_layer_neurons = 2
hidden_layer_neurons = 4
output_neurons = 1

# Random weights
hidden_weights = np.random.uniform(size=(input_layer_neurons, hidden_layer_neurons))
hidden_bias = np.random.uniform(size=(1, hidden_layer_neurons))
output_weights = np.random.uniform(size=(hidden_layer_neurons, output_neurons))
output_bias = np.random.uniform(size=(1, output_neurons))

# Learning rate and iterations
learning_rate = 0.5
epochs = 10000

# Training algorithm
for epoch in range(epochs):
    # Forward propagation
    hidden_layer_input = np.dot(X, hidden_weights) + hidden_bias
    hidden_layer_output = sigmoid(hidden_layer_input)

    output_layer_input = np.dot(hidden_layer_output, output_weights) + output_bias
    predicted_output = sigmoid(output_layer_input)

    # Backpropagation
    error = y - predicted_output
    d_predicted_output = error * sigmoid_derivative(predicted_output)

    error_hidden_layer = d_predicted_output.dot(output_weights.T)
    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)

    # Updating weights and biases
    output_weights += hidden_layer_output.T.dot(d_predicted_output) * learning_rate
    output_bias += np.sum(d_predicted_output, axis=0, keepdims=True) * learning_rate
    hidden_weights += X.T.dot(d_hidden_layer) * learning_rate
    hidden_bias += np.sum(d_hidden_layer, axis=0, keepdims=True) * learning_rate

# Final output
print("Final predicted output after training:")
print(np.round(predicted_output, 3))

"""
Theory Explanation:

What the program does:
This program trains a neural network using the backpropagation algorithm to learn the XOR function. 
It uses one hidden layer, and the sigmoid activation function is employed in both the hidden and output layers.

Key operations of the program:
1. **Initialization**:
   - The input, hidden, and output layer sizes are defined.
   - Random weights and biases are initialized for both layers.
   
2. **Forward propagation**:
   - The input is multiplied by the hidden layer weights, and biases are added to compute the hidden layer activations.
   - The activations of the hidden layer are then passed through the sigmoid function.
   - The same process is repeated for the output layer, where the output is predicted.

3. **Backpropagation**:
   - The error between the predicted and actual output is computed.
   - The derivative of the sigmoid function is used to calculate gradients, which represent how much each weight contributed to the error.
   - The weights and biases are updated using gradient descent with a learning rate.

4. **Weight Update**:
   - The gradients are used to adjust the weights and biases of both layers to minimize the error.

Potential real-life use cases:
- **Image classification**: A network like this could be used to classify simple images into two categories.
- **Logic gate simulation**: Similar networks can model binary logical operations (AND, OR, NOT) beyond XOR.
- **Pattern recognition**: Can be used in recognizing binary patterns in datasets.

Sample input/output scenario with explanation:
Input:
    [0, 0]
    [0, 1]
    [1, 0]
    [1, 1]

Expected Output:
    [0]   (XOR of 0, 0)
    [1]   (XOR of 0, 1)
    [1]   (XOR of 1, 0)
    [0]   (XOR of 1, 1)

Explanation:
The neural network learns the XOR function over 10,000 epochs. Each epoch updates the weights based on the error in predictions, and the model gradually becomes better at approximating the correct XOR values. After training, the predicted output closely matches the expected XOR results (0 for (0, 0) and (1, 1), and 1 for (0, 1) and (1, 0)).
"""
