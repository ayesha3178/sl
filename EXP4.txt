import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.datasets import fetch_california_housing  # Alternative dataset


# Load the dataset
california = fetch_california_housing()

# Convert it to a pandas DataFrame
df = pd.DataFrame(california.data, columns=california.feature_names)

# Add the target variable (house prices)
df['PRICE'] = california.target

# Display the first few rows
print(df.head())



# Check for missing values
print(df.isnull().sum())

# Summary statistics
print(df.describe())

# Visualize correlation between features and target variable
plt.figure(figsize=(10, 6))
sns.heatmap(df.corr(), annot=True, cmap="coolwarm", fmt=".2f")
plt.show()



# Define features (X) and target variable (y)
X = df.drop(columns=['PRICE'])
y = df['PRICE']

# Split data into 80% training and 20% testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Create Linear Regression Model
model = LinearRegression()

# Train the model
model.fit(X_train, y_train)

# Predict house prices on test data
y_pred = model.predict(X_test)

# Calculate performance metrics
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print(f"Mean Absolute Error: {mae:.2f}")
print(f"Mean Squared Error: {mse:.2f}")
print(f"Root Mean Squared Error: {rmse:.2f}")
print(f"R-squared Score: {r2:.2f}")

plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred, alpha=0.7, color='blue')
plt.xlabel("Actual Prices")
plt.ylabel("Predicted Prices")
plt.title("Actual Prices vs Predicted Prices")
plt.show()

'''
Theory Explanation:

What the program does:
This Python program builds a simple linear regression model to predict housing prices using the California Housing dataset. It involves loading the dataset, preprocessing, visualizing correlations, training a regression model, and evaluating the model's performance using key metrics and visualization.

Key operations of the program:
1. **Data Loading and Preparation**: The dataset is fetched using `fetch_california_housing()` and converted into a pandas DataFrame. The target variable (house price) is added as a new column.
2. **Data Exploration**: The program checks for missing values, displays summary statistics, and visualizes the correlation matrix using a heatmap to understand feature relationships.
3. **Data Splitting**: Features (X) and target (y) are separated. The dataset is split into training and test sets using an 80-20 ratio.
4. **Model Training**: A linear regression model is created and trained using the training data.
5. **Prediction and Evaluation**: Predictions are made on the test set, and the model's accuracy is assessed using MAE, MSE, RMSE, and R² score.
6. **Visualization**: A scatter plot of actual vs predicted prices is plotted to visually assess model performance.

Observations/Conclusions:
- The dataset had no missing values, allowing smooth model training.
- The heatmap shows how strongly each feature correlates with housing prices, which helps in understanding influential predictors.
- Linear regression provides a baseline model. The R² score gives an idea of how well the model explains the variability of the target.
- Visualization of actual vs predicted prices helps in identifying model bias or spread of residuals.
- While linear regression is easy to interpret, its performance may be limited if the data has nonlinear patterns or outliers.

'''
