from mrjob.job import MRJob
import re

LOG_PATTERN = r'\[(.?)\] \[(.?)\] \[(.?)\] (.)'

class LogAnalyzer(MRJob):
    def mapper(self, _, line):
        match = re.match(LOG_PATTERN, line)
        if match:
            timestamp, level, user, message = match.groups()
            yield (level.strip(), 1)

    def reducer(self, level, counts):
        yield (level, sum(counts))

if __name__ == '__main__':
    LogAnalyzer.run()

"""
Theory Explanation:

What the program does:
The program is a MapReduce job using the MRJob library to analyze log data. It processes a log file line by line, extracting log levels (such as "INFO", "ERROR", "WARNING", etc.) and counts how many times each level appears. The job then aggregates these counts for each log level.

Key operations of the program:
1. **Mapper**:
   - Each line from the input file is processed.
   - A regular expression (`LOG_PATTERN`) is used to match and extract the timestamp, log level, user, and message from the log line.
   - The `mapper` yields a key-value pair where the key is the log level and the value is `1` (indicating a single occurrence of that log level).
   
2. **Reducer**:
   - The reducer receives the log level (key) and a list of counts (values).
   - It sums the counts for each log level and yields the total count for that level.

Observations/Conclusions:
- **Input Format**: The log lines are assumed to be in the format `"[timestamp] [log_level] [user] message"`, where each part is enclosed in square brackets.
- **Output**: The final output will be a count of how many times each log level appears in the input dataset.
  - Example output could look like:
    - `INFO 10`
    - `ERROR 3`
    - `WARNING 5`
- **Regular Expression**: The `LOG_PATTERN` regex is designed to capture the four parts of each log entry: timestamp, log level, user, and message. This allows the program to extract and process the log level for counting.
- **Efficiency**: The program utilizes MapReduce's distributed processing, which makes it scalable and efficient for processing large amounts of log data. The `mapper` and `reducer` functions are designed to work in parallel across multiple machines in a distributed system.
"""
