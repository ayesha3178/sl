import nltk
import string
import numpy as np
import pandas as pd
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk import pos_tag
from sklearn.feature_extraction.text import TfidfVectorizer

# Download required NLTK datasets
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')


text = """Text analytics is the process of deriving meaningful insights from textual data. 
It involves techniques such as tokenization, stemming, lemmatization, and TF-IDF. 
Natural Language Processing (NLP) plays a crucial role in text analytics."""
tokens = word_tokenize(text)  # Tokenizing words
print("Tokenized Words:", tokens)


pos_tags = pos_tag(tokens)
print("POS Tagging:", pos_tags)
stop_words = set(stopwords.words('english'))
filtered_tokens = [word for word in tokens if word.lower() not in stop_words and word not in string.punctuation]
print("After Stop Words Removal:", filtered_tokens)


stemmer = PorterStemmer()
stemmed_words = [stemmer.stem(word) for word in filtered_tokens]
print("After Stemming:", stemmed_words)


lemmatizer = WordNetLemmatizer()
lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_tokens]
print("After Lemmatization:", lemmatized_words)


documents = [
    "Text analytics is important for deriving insights.",
    "Natural Language Processing and text analytics help in understanding textual data.",
    "Tokenization, stemming, and lemmatization are key preprocessing steps in NLP."
]

vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(documents)

# Convert matrix to readable format
tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())

print("\nTF-IDF Matrix:")
print(tfidf_df)

'''
Theory Explanation:

What the program does:
This program demonstrates fundamental techniques in Natural Language Processing (NLP), including tokenization, stop word removal, stemming, lemmatization, part-of-speech (POS) tagging, and TF-IDF vectorization. These are common preprocessing steps for analyzing textual data.

Key operations of the program:
1. **Tokenization**: The input text is split into individual words using NLTK’s `word_tokenize` function.
2. **POS Tagging**: Each token is assigned a part-of-speech label (e.g., noun, verb) to understand grammatical structure.
3. **Stop Words Removal**: Commonly used words that do not contribute much meaning (e.g., "the", "is") are filtered out.
4. **Stemming**: Words are reduced to their root form using the Porter Stemmer (e.g., "processing" → "process").
5. **Lemmatization**: Words are converted to their base form using the WordNet lemmatizer (e.g., "deriving" → "derive").
6. **TF-IDF Vectorization**: Three text documents are transformed into numerical representations using Term Frequency–Inverse Document Frequency (TF-IDF), which highlights important terms while down-weighting commonly occurring ones.

Observations/Conclusions:
- Tokenization and stop word removal help reduce noise in textual data.
- Stemming and lemmatization are both used to normalize words, though lemmatization is often more accurate linguistically.
- POS tagging helps in understanding syntactic structures and can be useful in downstream NLP tasks.
- TF-IDF effectively quantifies the relevance of terms in documents, useful for text classification, clustering, or information retrieval.
- These preprocessing steps are essential for converting unstructured text into a format suitable for machine learning models and data analysis.

'''
