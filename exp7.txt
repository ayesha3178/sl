import numpy as np

# Sigmoid activation function
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Derivative of sigmoid function
def sigmoid_derivative(x):
    return x * (1 - x)

# XOR input and output
inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
outputs = np.array([[0], [1], [1], [0]])

# Seed for reproducibility
np.random.seed(42)

# Initialize weights and biases
input_layer_neurons = 2
hidden_layer_neurons = 2
output_neurons = 1

# Weights and biases
weights_input_hidden = np.random.uniform(size=(input_layer_neurons, hidden_layer_neurons))
weights_hidden_output = np.random.uniform(size=(hidden_layer_neurons, output_neurons))
bias_hidden = np.random.uniform(size=(1, hidden_layer_neurons))
bias_output = np.random.uniform(size=(1, output_neurons))

# Training parameters
learning_rate = 0.1
epochs = 10000

# Training the network
for epoch in range(epochs):
    # Forward pass
    hidden_layer_activation = np.dot(inputs, weights_input_hidden) + bias_hidden
    hidden_layer_output = sigmoid(hidden_layer_activation)
    output_layer_activation = np.dot(hidden_layer_output, weights_hidden_output) + bias_output
    predicted_output = sigmoid(output_layer_activation)

    # Error calculation
    error = outputs - predicted_output

    # Backpropagation
    d_predicted_output = error * sigmoid_derivative(predicted_output)
    error_hidden_layer = d_predicted_output.dot(weights_hidden_output.T)
    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)

    # Updating weights and biases
    weights_hidden_output += hidden_layer_output.T.dot(d_predicted_output) * learning_rate
    bias_output += np.sum(d_predicted_output, axis=0, keepdims=True) * learning_rate
    weights_input_hidden += inputs.T.dot(d_hidden_layer) * learning_rate
    bias_hidden += np.sum(d_hidden_layer, axis=0, keepdims=True) * learning_rate

# Testing the trained network
print("Final outputs after training:")
print(predicted_output.round())

"""
Theory Explanation:

What the program does:
This Python program implements a simple feedforward neural network with one hidden layer to solve the XOR logic problem.
It uses the sigmoid activation function and the backpropagation algorithm to train the network to learn the XOR mapping.

The key operations of the program:
1. **Network Architecture**:
   - Input layer: 2 neurons (for the two XOR inputs).
   - Hidden layer: 2 neurons (with sigmoid activation).
   - Output layer: 1 neuron (with sigmoid activation to produce a value between 0 and 1).

2. **Initialization**:
   - Weights and biases for both layers are initialized randomly.

3. **Forward Propagation**:
   - Computes activations for hidden and output layers using the sigmoid function.

4. **Error Calculation**:
   - The difference between the actual output and expected XOR output is calculated.

5. **Backpropagation**:
   - Computes gradients of error with respect to weights using the chain rule.
   - Updates weights and biases using gradient descent.

6. **Training Loop**:
   - Repeats forward and backward propagation for 10,000 epochs to reduce the error.

7. **Output**:
   - After training, the model predicts the XOR outputs which are then printed.

Potential real-life use cases:
- XOR is a benchmark problem for testing neural networks' ability to learn non-linearly separable functions.
- Building block for understanding complex classification tasks.
- Foundation for deep learning models used in tasks like image recognition, language translation, etc.

A sample input/output scenario with an explanation:
Input : [0, 0] → Output: ~0  
Input : [0, 1] → Output: ~1  
Input : [1, 0] → Output: ~1  
Input : [1, 1] → Output: ~0

Explanation:
The XOR function returns 1 when only one of the inputs is 1, and 0 otherwise.
The trained network learns this behavior by adjusting the weights and biases during training,
so that the final predicted outputs match the XOR truth table closely.
"""
